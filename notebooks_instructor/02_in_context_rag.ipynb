{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-context learning and RAG\n",
    "This is the second notebook for the LLM section of Comp 255.\n",
    "\n",
    "Sections:\n",
    "* System prompts\n",
    "* Zero-shot prompting\n",
    "* Few-shot prompting\n",
    "* Structured outputs\n",
    "* Memory\n",
    "* RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import SimpleBot, StructuredBot, ChatBot\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "\n",
    "sft_model = \"qwen2.5:1.5b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System prompts\n",
    "Depending on the model, the \"system prompt\" section is handled a little differently from the instruction itself.  \n",
    "\n",
    "You can see the \"system\" tag in Ollama's [template for Llama3](https://ollama.com/library/llama3/blobs/8ab4849b038c).  This is where the prompts we put below will be inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surfer = SimpleBot(\n",
    "    system_prompt='Respond like a surfer dude',\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    ")\n",
    "\n",
    "pirate = SimpleBot(\n",
    "    system_prompt='Respond like a pirate',\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    ")\n",
    "\n",
    "print(surfer.system_prompt)\n",
    "print(pirate.system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = surfer('How are you today?')\n",
    "print('\\n')\n",
    "response = pirate('How are you today?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot prompting\n",
    "\"Zero-shot\" learning refers to a model's ability to provide a correct output to a question it wasn't directly trained to answer.  We already sort of have an example of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_bot = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "  model_name=f\"ollama_chat/{sft_model}\",\n",
    ")\n",
    "\n",
    "response = simple_bot('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we want it to JUST output the name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the following question, provide no other information:\n",
    "What is the capital of France?\"\"\"\n",
    "response = simple_bot(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot prompting\n",
    "The above is fine, but maybe we want our output in a particular format.  We'd be best served by giving the model examples.  This is the \"few\" in \"few-shot\" - we're giving the model examples rather than just instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the following question in the following format:\n",
    "\n",
    "Question: What is the capital of Germany?\n",
    "Answer: Berlin\n",
    "\n",
    "Question: What is the capital of France?\n",
    "Answer: \"\"\"\n",
    "response = simple_bot(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be useful to have it output in JSON format for easier parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the following question in JSON format:\n",
    "\n",
    "Question: What is the capital of Germany?\n",
    "Answer: {\"country\": \"Germany\", \"capital\": \"Berlin\"}\n",
    "\n",
    "Question: What is the capital of France?\n",
    "Answer:\"\"\"\n",
    "response = simple_bot(prompt)\n",
    "\n",
    "json.loads(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your settings, the above may fail already!\n",
    "\n",
    "What if we wanted something a bit more complicated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the following question in JSON format:\n",
    "\n",
    "Question: Tell me about Germany\n",
    "Answer: {\n",
    "    \"country\": \"Germany\", \n",
    "    \"capital\": \"Berlin\",\n",
    "    \"language\": \"German\",}\n",
    "\n",
    "Question: Tell me about France\"\"\"\n",
    "response = simple_bot(prompt)\n",
    "\n",
    "json.loads(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will usually fail because it will not produce valid JSON or it will produce something besides JUST parsable JSON.  That's where structured outputs are useful! \n",
    "\n",
    "## Structured Output\n",
    "This is implemented differently with different vendors, but with Ollama, the supplied structure is converted into a \"grammar\" which defines which tokens are valid and which are not.  Based on that, invalid token predictions are ignored and only the (hopefully) valid response is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Country(BaseModel):\n",
    "  # source: https://ollama.com/blog/structured-outputs\n",
    "  name: str\n",
    "  capital: str\n",
    "  languages: list[str]\n",
    "\n",
    "struct_completer = StructuredBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    "    pydantic_model=Country,\n",
    ")\n",
    "\n",
    "response = struct_completer('Tell me about France')\n",
    "\n",
    "json.loads(response.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbots\n",
    "You'll notice that everything we've done so far is a single request, single response.  There is no \"conversation\" and that's because the model has no context; it doesn't remember previous inputs or responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate = SimpleBot(\n",
    "    system_prompt='You are a pirate',\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pirate('How are you today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pirate('What did you just say?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `Llamabot.ChatBot`, we provide the model with context (through the `messages` attribute).  This is inserted into the input to the model and it generates a response that is context-sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate_chat = ChatBot(\n",
    "  \"You are a pirate\",\n",
    "  session_name=\"pirate_chat\",  \n",
    "  model_name=f\"ollama_chat/{sft_model}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pirate_chat.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pirate_chat('How are you today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pirate_chat.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pirate_chat('What did you say?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate_chat.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG)\n",
    "You've likely heard some buzz about this concept.  There's a lot of complex ways to implement this, but the basic version is essentially just providing the model context based on the product of a \"retrieval\" workflow.\n",
    "\n",
    "Let's use the above as an example.  We're relying on the model's internal knowledge to give us the correct information. This doesn't always work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = struct_completer('Tell me about Papua New Guinea')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look up on [Wikipedia](https://en.wikipedia.org/wiki/Languages_of_Papua_New_Guinea), we get a different answer:\n",
    "\n",
    "\"Languages with statutory recognition are Tok Pisin, English, Hiri Motu, and Papua New Guinean Sign Language...\" \n",
    "\n",
    "So what if we inserted that into our prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_completer(\"\"\"\n",
    "Here's some useful context: Languages with statutory recognition are Tok Pisin, English, Hiri Motu, and Papua New Guinean Sign Language\n",
    "                 \n",
    "Tell me about Papua New Guinea\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guess what, you just did RAG! But I'm guessing you probably don't want to always be the \"R\" part of the workflow.  In that case, we need to set up automated retrieval and to that we need to set up a document store!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector stores\n",
    "The first part of RAG is \"retrieval\".  To do that we essentially need to create a mechanism for the model to retrieve relevant information.  One approach is to create a set of \"embeddings\" for our documents that can be compared against the embedding of an input prompt.\n",
    "\n",
    "First let's create some documents.  Let's say one contains information about Papua New Guinea, another contains information about France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Languages with statutory recognition in Papua New Guinea \\\n",
    "are Tok Pisin, English, Hiri Motu, and \\\n",
    "Papua New Guinean Sign Language\"\n",
    "\n",
    "doc2 = \"The only language with statutory recognition in France \\\n",
    "is French\"\n",
    "\n",
    "# write these to a temporary file\n",
    "with open(\"doc1.txt\", \"w\") as f:\n",
    "    f.write(doc1)\n",
    "\n",
    "with open(\"doc2.txt\", \"w\") as f:\n",
    "    f.write(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using an implementation from Llamabot, which uses [LanceDB](https://lancedb.com/) on the backend.  LanceDB has a nice implementation for creating vector stores.  Let's see how that looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "\n",
    "# create a database\n",
    "db = lancedb.connect(\"/tmp/db\")\n",
    "# initialize a default sentence-transformers model (paraphrase-MiniLM-L6-v2)\n",
    "model = get_registry().get(\"sentence-transformers\").create()\n",
    "\n",
    "# specify a schema (just text + vector)\n",
    "class Words(LanceModel):\n",
    "    text: str = model.SourceField()\n",
    "    vector: Vector(model.ndims()) = model.VectorField()\n",
    "\n",
    "\n",
    "try:\n",
    "    table = db.create_table(\"words\", schema=Words)\n",
    "except ValueError:\n",
    "    table = db.open_table(\"words\")\n",
    "\n",
    "# add in some entries\n",
    "table.add(\n",
    "    [\n",
    "        {\"text\": \"hello world\"},\n",
    "        {\"text\": \"goodbye world\"}\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the entries\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"greetings\"\n",
    "search_query = table.search(query)\n",
    "search_query._query[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a single (most similar) result, translate it into the pydantic model\n",
    "search_query.limit(1).to_pydantic(Words)[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"farewell\"\n",
    "result = table.search(query).limit(1).to_pydantic(Words)[0]\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"random word\"\n",
    "result = table.search(query).limit(1).to_pydantic(Words)[0]\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamabot provides a class called `QueryBot` which implements everything above for you and allows you to just query the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import QueryBot\n",
    "from pathlib import Path\n",
    "\n",
    "system_message = \"You are a helpful assistant that can answer questions \\\n",
    "    based on the provided documents.\"\n",
    "doc_paths = [Path(\"doc1.txt\"), Path(\"doc2.txt\")]\n",
    "\n",
    "query_completer = QueryBot(\n",
    "    system_prompt=system_message,\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    "    collection_name=\"documents\",\n",
    "    document_paths=doc_paths\n",
    ")\n",
    "print(query_completer.system_prompt.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Tell me about Papua New Guinea\"\n",
    "# what does it retrieve (default n of results is 20)\n",
    "print(query_completer.docstore.retrieve(q))\n",
    "response = query_completer('Tell me about Papua New Guinea, very brief')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_completer('Tell me about France, very brief')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'Tell me about Germany'\n",
    "query_completer.docstore.retrieve(q, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_completer('Tell me about Germany, very brief')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
