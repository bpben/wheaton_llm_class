{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Agents\n",
    "This is the third notebook for the LLM section of Comp 255.\n",
    "\n",
    "Sections:\n",
    "* Is RAG an agent?\n",
    "* Tool calling with AgentBot\n",
    "* RAG + AgentBot\n",
    "* (Optional) Fine-tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import QueryBot, AgentBot, tool\n",
    "from pathlib import Path\n",
    "\n",
    "sft_model = \"qwen2.5:1.5b\"\n",
    "agent_model = \"qwen2.5:7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the data back up\n",
    "doc1 = \"Languages with statutory recognition in Papua New Guinea \\\n",
    "are Tok Pisin, English, Hiri Motu, and \\\n",
    "Papua New Guinean Sign Language\"\n",
    "\n",
    "doc2 = \"The only language with statutory recognition in France \\\n",
    "is French\"\n",
    "\n",
    "# write these to a temporary file\n",
    "with open(\"doc1.txt\", \"w\") as f:\n",
    "    f.write(doc1)\n",
    "\n",
    "with open(\"doc2.txt\", \"w\") as f:\n",
    "    f.write(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant that answers questions \\\n",
    "    based on the provided documents.\"\n",
    "doc_paths = [Path(\"doc1.txt\"), Path(\"doc2.txt\")]\n",
    "\n",
    "query_completer = QueryBot(\n",
    "    system_prompt=system_message,\n",
    "    model_name = f\"ollama_chat/{sft_model}\",\n",
    "    collection_name=\"documents\",\n",
    "    document_paths=doc_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_completer('Hi how are you today!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this an agent? It did respond reasonably, but it seems to be pretty preoccupied with the RAG use-case.  An agent should have \"agency\"; it should be able to decide the right action to take.  Sometimes that might involve the RAG workflow, but other times it won't.\n",
    "\n",
    "Retrieval can be thought of as a tool that the agent can choose to use.  One way to specify that is to create a function that does the retrieving.  Then, through natural language, we can describe the tool and the agent can decide if it makes sense to use it to respond to an input.  \n",
    "\n",
    "Let's look at tool use more generally using `AgentBot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/ericmjl/llamabot/blob/main/docs/tutorials/agentbot.md\n",
    "# llamabot.tool decorator\n",
    "@tool\n",
    "def calculate_total_with_tip(bill_amount: float, tip_rate: float) -> float:\n",
    "    if tip_rate < 0 or tip_rate > 1.0:\n",
    "        raise ValueError(\"Tip rate must be between 0 and 1.0\")\n",
    "    return bill_amount * (1 + tip_rate)\n",
    "\n",
    "# Create the bot\n",
    "bot = AgentBot(\n",
    "    system_prompt=\"You are my assistant with respect to restaurant bills.\",\n",
    "    functions=[calculate_total_with_tip, ],\n",
    "    model_name=f\"ollama_chat/{agent_model}\",\n",
    "    #model_name='gpt-4o'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at what is being provided to the model\n",
    "print(calculate_total_with_tip.json_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system prompt has been extended with additional information\n",
    "bot.decision_bot.system_prompt.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_total_only_prompt = \"My dinner was $2300 without tips. Calculate my total with an 18% tip.\"\n",
    "response = bot(calculate_total_only_prompt, max_iterations=4)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will *usually* work.  The issue with small models (7 billion parameters in this case) is that they can sometimes output strange things, which will cause errors.  Sometimes the models can self-correct, but sometimes not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bot now could be said to have \"agency\"; it can decide whether to use a tool or just go ahead and respond normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bot('Hello how are you?')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make the RAG workflow a tool.  Then we can use it to answer our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bot within a bot - you can likely replace this with the LanceDB query itself\n",
    "system_message = \"You are a helpful assistant that can answer questions \\\n",
    "    based on the provided documents.\"\n",
    "doc_paths = [Path(\"doc1.txt\"), Path(\"doc2.txt\")]\n",
    "query_completer = QueryBot(\n",
    "    system_prompt=system_message,\n",
    "    model_name=f\"ollama_chat/{agent_model}\",\n",
    "    collection_name=\"documents\",\n",
    "    document_paths=doc_paths,\n",
    ")\n",
    "\n",
    "@tool\n",
    "def information_retrieval(query: str) -> str:\n",
    "    query_completer(query)\n",
    "\n",
    "# Create the bot\n",
    "bot = AgentBot(\n",
    "    system_prompt=\"You are my assistant and can retrieve information if needed.\",\n",
    "    functions=[information_retrieval],\n",
    "    model_name=f\"ollama_chat/{sft_model}\",   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bot(\"Hello how are you?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bot(\"What are the national languages of Papua New Guinea\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see some oddities, but we've managed to roughly get our RAG workflow to be an tool that the agent is able to make a decision to call with specific parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
