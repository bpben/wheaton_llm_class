{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LLMs (Student version)\n",
    "This is the first notebook for the LLM section of Comp 255.  It provides a quick intro to some of the basics of using LLMs.\n",
    "\n",
    "1. [Setup required environment](#setup-required-environment)\n",
    "    Refer to the README.md for instructions here\n",
    "2. [Pre-trained models](#pre-trained-models)\n",
    "    - [Temperature](#temperature)\n",
    "    - [Exercise - what is missing?](#exercise---what-is-missing)\n",
    "3. [Instruction tuned models](#instruction-tuned-models)\n",
    "    - [Exercise - experimentation with parameters](#exercise---experimentation-with-parameters)\n",
    "    - [Exercise - what ELSE is missing?](#exercise---what-else-is-missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/llamabot/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llamabot import SimpleBot\n",
    "pretrained_model = 'llama3.2:1b-text-q5_K_S'\n",
    "sft_model = \"qwen2.5:1.5b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained models\n",
    "Right now we're using a simple \"pre-trained\" version of the Mistral model.  It's just been trained on the language modeling objective; it learns to predict the next word.  As a result, you can see the ouput just continues the input text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "completer = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "  model_name=f\"ollama_chat/{pretrained_model}\",\n",
    "  temperature=0.0,\n",
    "  num_predict=50\n",
    ")\n",
    "\n",
    "response = completer('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature \n",
    "Temperature controls how much \"randomness\" there is in prediction.  Low temperatures makes the model predict likely tokens, resulting in sequences closer to its training data.  High temperatures means the model will predict tokens less like its training data.  Low = more stable, consistent answers.  High = more random answers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completer = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "  model_name=f\"ollama_chat/{pretrained_model}\",\n",
    "  temperature=100.0,\n",
    "  num_predict=50\n",
    ")\n",
    "\n",
    "response = completer('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - what is missing?\n",
    "You've probably used ChatGPT before.  What is missing from this bot? Experiment with having a conversation and note down what is missing.\n",
    "\n",
    "* Does the bot answer your questions?\n",
    "* What does the bot seem to be doing? Think about how this model is trained\n",
    "* Does the bot seem to recall what it previously said to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction tuned models\n",
    "Usually the above won't give us useful answers.  That's because the model is not tuned to produce useful answers, just to predict the next word.  \n",
    "\n",
    "That's where instruction tuning comes in.  That's covered in the slides, but here we'll re-run some of the above with a model that has been instruction tuned (Llama3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note - using default temperature (0.0) and no predict limit\n",
    "# instruction tuned are better at knowing when to shush\n",
    "completer = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    ")\n",
    "\n",
    "response = completer('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completer('Hello, how are you?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - experimentation with parameters\n",
    "Take a look at the SimpleBot docstring.  What are the parameters you could experiment with?\n",
    "\n",
    "* Alter the system prompt.  What is the effect?\n",
    "* Alter the temperature.  What is the effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Bot that is primed with a system prompt, accepts a human message,\n",
      "    and sends back a single response.\n",
      "\n",
      "    This bot does not retain chat history.\n",
      "\n",
      "    :param system_prompt: The system prompt to use.\n",
      "    :param temperature: The model temperature to use.\n",
      "        See https://platform.openai.com/docs/api-reference/completions/create#completions/create-temperature\n",
      "        for more information.\n",
      "    :param model_name: The name of the model to use.\n",
      "    :param stream_target: The target to stream the response to.\n",
      "        Should be one of (\"stdout\", \"panel\", \"api\").\n",
      "    :param json_mode: Whether to print debug messages.\n",
      "    :param api_key: The OpenAI API key to use.\n",
      "    :param mock_response: A mock response to use, for testing purposes only.\n",
      "    :param completion_kwargs: Additional keyword arguments to pass to the completion function.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(SimpleBot.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - what ELSE is missing?\n",
    "So this instruction-tuned model is better at answering questions.  But what ELSE is missing here that makes it unlike ChatGPT?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
