{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-context learning and RAG\n",
    "This is the second notebook for the LLM section of Comp 255.\n",
    "\n",
    "1. [System prompts](#system-prompts)\n",
    "2. [Zero-shot prompting](#zero-shot-prompting)\n",
    "3. [Few-shot prompting](#few-shot-prompting)\n",
    "    - [Exercise: Writing JSON](#exercise-writing-json)\n",
    "4. [Structured outputs](#structured-outputs)\n",
    "5. [Chatbots](#chatbots)\n",
    "6. [Retrieval Augmented Generation (RAG)](#retrieval-augmented-generation-rag)\n",
    "    - [Vector stores](#vector-stores)\n",
    "    - [Exercise: Your turn!](#exercise-your-turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import SimpleBot, StructuredBot, ChatBot\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "\n",
    "sft_model = \"qwen2.5:1.5b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System prompts\n",
    "Depending on the model, the \"system prompt\" section is handled a little differently from the instruction itself.  \n",
    "\n",
    "You can see the \"system\" tag in Ollama's [template for Llama3](https://ollama.com/library/llama3/blobs/8ab4849b038c).  This is where the prompts we put below will be inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surfer = SimpleBot(\n",
    "    system_prompt='Respond like a surfer dude',\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    ")\n",
    "\n",
    "pirate = SimpleBot(\n",
    "    system_prompt='Respond like a pirate',\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    ")\n",
    "\n",
    "print(surfer.system_prompt)\n",
    "print(pirate.system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = surfer('How are you today?')\n",
    "print('\\n')\n",
    "response = pirate('How are you today?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot prompting\n",
    "\"Zero-shot\" learning refers to a model's ability to provide a correct output to a question it wasn't directly trained to answer.  We already sort of have an example of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_bot = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "  model_name=f\"ollama_chat/{sft_model}\",\n",
    ")\n",
    "\n",
    "response = simple_bot('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we want it to JUST output the name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the following question, provide no other information:\n",
    "What is the capital of France?\"\"\"\n",
    "response = simple_bot(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot prompting\n",
    "The above is fine, but maybe we want our output in a particular format.  We'd be best served by giving the model examples.  This is the \"few\" in \"few-shot\" - we're giving the model examples rather than just instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the following question in the following format:\n",
    "\n",
    "Question: What is the capital of Germany?\n",
    "Answer: Berlin\n",
    "\n",
    "Question: What is the capital of France?\n",
    "Answer: \"\"\"\n",
    "response = simple_bot(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Writing JSON\n",
    "There may be the case that you want the LLM to output something with some defined structure so you can apply functions neatly on top of it.  One such format is JSON.  \n",
    "\n",
    "Say you want an output for country information in the following format:\n",
    "\n",
    "`{\"country\": str, \"capital\": str}`\n",
    "\n",
    "Ask your bot to do this for you! Try in zero shot formulation and few-shot formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Output\n",
    "As you can see, sometimes these models are iffy about producing valid, parseable JSON.  Even ChatGPT suffers from this, sometimes outputting some preamble that wouldn't, without processing, be parseable.  That's why it makes sense to think about \"structured output\", which can be implemented in different ways. In Ollama, the supplied structure is converted into a \"grammar\" which defines which tokens are valid and which are not.  Based on that, invalid token predictions are ignored and only the (hopefully) valid response is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Country(BaseModel):\n",
    "  # source: https://ollama.com/blog/structured-outputs\n",
    "  name: str\n",
    "  capital: str\n",
    "  languages: list[str]\n",
    "\n",
    "struct_completer = StructuredBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    "    pydantic_model=Country,\n",
    ")\n",
    "\n",
    "response = struct_completer('Tell me about France')\n",
    "\n",
    "json.loads(response.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbots\n",
    "You'll notice that everything we've done so far is a single request, single response.  There is no \"conversation\" and that's because the model has no context; it doesn't remember previous inputs or responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate = SimpleBot(\n",
    "    system_prompt='You are a pirate',\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pirate('How are you today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pirate('What did you just say?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `Llamabot.ChatBot`, we provide the model with context (through the `messages` attribute).  This is inserted into the input to the model and it generates a response that is context-sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate_chat = ChatBot(\n",
    "  \"You are a pirate\",\n",
    "  session_name=\"pirate_chat\",  \n",
    "  model_name=f\"ollama_chat/{sft_model}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pirate_chat.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pirate_chat('How are you today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pirate_chat.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pirate_chat('What did you say?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate_chat.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG)\n",
    "You've likely heard some buzz about this concept.  There's a lot of complex ways to implement this, but the basic version is essentially just providing the model context based on the product of a \"retrieval\" workflow.\n",
    "\n",
    "Let's use the above as an example.  We're relying on the model's internal knowledge to give us the correct information. This doesn't always work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = struct_completer('Tell me about Papua New Guinea')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look up on [Wikipedia](https://en.wikipedia.org/wiki/Languages_of_Papua_New_Guinea), we get a different answer:\n",
    "\n",
    "\"Languages with statutory recognition are Tok Pisin, English, Hiri Motu, and Papua New Guinean Sign Language...\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Make my model smarter\n",
    "Think about how we might be able to ensure the model produces the right answer here.  Experiment and see if you can get it to generate the right answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_completer(\"\"\"\n",
    "Here's some useful context: Languages with statutory recognition are Tok Pisin, English, Hiri Motu, and Papua New Guinean Sign Language\n",
    "                 \n",
    "Tell me about Papua New Guinea\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guess what, you just did RAG! But I'm guessing you probably don't want to always be the \"R\" part of the workflow.  In that case, we need to set up automated retrieval and to that we need to set up a document store!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector stores\n",
    "The first part of RAG is \"retrieval\".  To do that we essentially need to create a mechanism for the model to retrieve relevant information.  One approach is to create a set of \"embeddings\" for our documents that can be compared against the embedding of an input prompt.\n",
    "\n",
    "First let's create some documents.  Let's say one contains information about Papua New Guinea, another contains information about France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Languages with statutory recognition in Papua New Guinea \\\n",
    "are Tok Pisin, English, Hiri Motu, and \\\n",
    "Papua New Guinean Sign Language\"\n",
    "\n",
    "doc2 = \"The only language with statutory recognition in France \\\n",
    "is French\"\n",
    "\n",
    "# write these to a temporary file\n",
    "with open(\"doc1.txt\", \"w\") as f:\n",
    "    f.write(doc1)\n",
    "\n",
    "with open(\"doc2.txt\", \"w\") as f:\n",
    "    f.write(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using an implementation from Llamabot, which uses [LanceDB](https://lancedb.com/) on the backend.  LanceDB has a nice implementation for creating vector stores.  Let's see how that looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "\n",
    "# create a database\n",
    "db = lancedb.connect(\"/tmp/db\")\n",
    "# initialize a default sentence-transformers model (paraphrase-MiniLM-L6-v2)\n",
    "model = get_registry().get(\"sentence-transformers\").create()\n",
    "\n",
    "# specify a schema (just text + vector)\n",
    "class Words(LanceModel):\n",
    "    text: str = model.SourceField()\n",
    "    vector: Vector(model.ndims()) = model.VectorField()\n",
    "\n",
    "\n",
    "try:\n",
    "    table = db.create_table(\"words\", schema=Words)\n",
    "except ValueError:\n",
    "    table = db.open_table(\"words\")\n",
    "\n",
    "# add in some entries\n",
    "table.add(\n",
    "    [\n",
    "        {\"text\": \"hello world\"},\n",
    "        {\"text\": \"goodbye world\"}\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the entries\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"greetings\"\n",
    "search_query = table.search(query)\n",
    "search_query._query[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a single (most similar) result, translate it into the pydantic model\n",
    "search_query.limit(1).to_pydantic(Words)[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"farewell\"\n",
    "result = table.search(query).limit(1).to_pydantic(Words)[0]\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"random word\"\n",
    "result = table.search(query).limit(1).to_pydantic(Words)[0]\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamabot provides a class called `QueryBot` which implements everything above for you and allows you to just query the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import QueryBot\n",
    "from pathlib import Path\n",
    "\n",
    "system_message = \"You are a helpful assistant that can answer questions \\\n",
    "    based on the provided documents.\"\n",
    "doc_paths = [Path(\"doc1.txt\"), Path(\"doc2.txt\")]\n",
    "\n",
    "query_completer = QueryBot(\n",
    "    system_prompt=system_message,\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    "    collection_name=\"documents\",\n",
    "    document_paths=doc_paths\n",
    ")\n",
    "print(query_completer.system_prompt.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Tell me about Papua New Guinea\"\n",
    "# what does it retrieve (default n of results is 20)\n",
    "print(query_completer.docstore.retrieve(q))\n",
    "response = query_completer('Tell me about Papua New Guinea, very brief')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_completer('Tell me about France, very brief')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'Tell me about Germany'\n",
    "query_completer.docstore.retrieve(q, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_completer('Tell me about Germany, very brief')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Your turn!\n",
    "Now that you have a sense of how to build out a simple database and the `QueryBot`, try this yourself.  You can either continue using this setup and use another complex country (e.g. Bolivia), or you can build your own document set and query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
