{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Agents\n",
    "This is the third notebook for the LLM section of Comp 255.\n",
    "\n",
    "1. [Is RAG an agent?](#is-rag-an-agent)\n",
    "2. [Tool use with AgentBot](#tool-use-with-agentbot)\n",
    "   - [Exercise: Make a RAG tool](#exercise-make-a-rag-tool)\n",
    "3. Fine-tuning: Access [this notebook](https://colab.research.google.com/drive/1HB5c6RkyOljXXghih11Y8Xbe51uhO2s2?usp=sharing) for this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/llamabot/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llamabot import QueryBot, AgentBot, tool\n",
    "from pathlib import Path\n",
    "\n",
    "sft_model = \"qwen2.5:1.5b\"\n",
    "agent_model = \"qwen2.5:7b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is RAG an agent?\n",
    "Let's set up our RAG workflow from last time and consider whether this meets the definition of \"agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the data back up\n",
    "doc1 = \"Languages with statutory recognition in Papua New Guinea \\\n",
    "are Tok Pisin, English, Hiri Motu, and \\\n",
    "Papua New Guinean Sign Language\"\n",
    "\n",
    "doc2 = \"The only language with statutory recognition in France \\\n",
    "is French\"\n",
    "\n",
    "# write these to a temporary file\n",
    "with open(\"doc1.txt\", \"w\") as f:\n",
    "    f.write(doc1)\n",
    "\n",
    "with open(\"doc2.txt\", \"w\") as f:\n",
    "    f.write(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant that answers questions \\\n",
    "    based on the provided documents.\"\n",
    "doc_paths = [Path(\"doc1.txt\"), Path(\"doc2.txt\")]\n",
    "\n",
    "query_completer = QueryBot(\n",
    "    system_prompt=system_message,\n",
    "    model_name = f\"ollama_chat/{sft_model}\",\n",
    "    collection_name=\"documents\",\n",
    "    document_paths=doc_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_completer('Hi how are you today!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this an agent? It did respond reasonably, but it seems to be pretty preoccupied with the RAG use-case.  An agent should have \"agency\"; it should be able to decide the right action to take.  Sometimes that might involve the RAG workflow, but other times it won't.\n",
    "\n",
    "## Tool use with AgentBot\n",
    "\n",
    "Just like you would use Google to figure out what languages are spoken in a particular country, the agent can be designed to use \"tools\" to answer a question.  In that case, the retrieval workflow above could just be considered a tool.\n",
    "\n",
    "A tool, in this case, is essentially just a function or set of functions.  Since the LLM can't directly run a function, it has to generate a response that can then be parsed into what function to call and what parameters to use.\n",
    "\n",
    "Here we create a simple tool for calculating the total of a restaurant bill with tip.  Then we create an `AgentBot` instance, which uses a specially-designed prompt with information about the available tools.  With this information, the LLM can decide whether or not to use a tool and, if it uses a tool, what parameters to supply.\n",
    "\n",
    "NOTE: This is a COMPLEX task and requires a bigger model than we've been using thus far.  Complex workflows are likely outside the capabilities of even this larger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/ericmjl/llamabot/blob/main/docs/tutorials/agentbot.md\n",
    "# llamabot.tool decorator\n",
    "@tool\n",
    "def calculate_total_with_tip(bill_amount: float, tip_rate: float) -> float:\n",
    "    if tip_rate < 0 or tip_rate > 1.0:\n",
    "        raise ValueError(\"Tip rate must be between 0 and 1.0\")\n",
    "    return bill_amount * (1 + tip_rate)\n",
    "\n",
    "# Create the bot\n",
    "bot = AgentBot(\n",
    "    system_prompt=\"You are my assistant with respect to restaurant bills.\",\n",
    "    functions=[calculate_total_with_tip, ],\n",
    "    model_name=f\"ollama_chat/{agent_model}\",\n",
    "    #model_name='gpt-4o'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'function_name': {'default': 'calculate_total_with_tip', 'description': 'Name of the function', 'title': 'Function Name', 'type': 'string'}, 'source_code': {'default': '@tool\\ndef calculate_total_with_tip(bill_amount: float, tip_rate: float) -> float:\\n    if tip_rate < 0 or tip_rate > 1.0:\\n        raise ValueError(\"Tip rate must be between 0 and 1.0\")\\n    return bill_amount * (1 + tip_rate)\\n', 'description': 'Source code of the function', 'title': 'Source Code', 'type': 'string'}, 'bill_amount': {'title': 'Bill Amount', 'type': 'number'}, 'tip_rate': {'title': 'Tip Rate', 'type': 'number'}}, 'required': ['bill_amount', 'tip_rate'], 'title': 'Calculate_Total_With_TipParameters', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "# look at what is being provided to the model\n",
    "print(calculate_total_with_tip.json_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_finish': <function llamabot.bot.agentbot.agent_finish(message: Any) -> str>,\n",
       " 'return_error': <function llamabot.bot.agentbot.return_error(message: Any) -> str>,\n",
       " 'calculate_total_with_tip': <function __main__.calculate_total_with_tip(bill_amount: float, tip_rate: float) -> float>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are my assistant with respect to restaurant bills. Given the following message and available tools, pick the next tool that you need to call and the arguments that you need for it. You have access to previously cached results - use them when appropriate by specifying their hash key in use_cached_results. Any arguments that you do not know ahead of time, just leave blank. Only call tools that are relevant to the user's message. If the task is complete, use the agent_finish tool.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# system prompt has been extended with additional information\n",
    "bot.decision_bot.system_prompt.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_total_only_prompt = \"My dinner was $2300 without tips. Calculate my total with an 18% tip.\"\n",
    "response = bot(calculate_total_only_prompt, max_iterations=4)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will *usually* work.  The issue with small models (7 billion parameters in this case) is that they can sometimes output strange things, which will cause errors.  Sometimes the models can self-correct, but sometimes not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bot now could be said to have \"agency\"; it can decide whether to use a tool or just go ahead and respond normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bot('Hello how are you?')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Make a RAG tool\n",
    "Since you already have the syntax above, you can wrap our previous retrieval workflow in a tool function.  Try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
